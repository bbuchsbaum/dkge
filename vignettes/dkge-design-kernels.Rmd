---
title: "Design Kernels and Model Tuning"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Design Kernels and Model Tuning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
set.seed(1)
```

This note focuses on how the design kernel steers DKGE fits and how to tune rank
or penalty parameters before running large analyses.

## Simulated Working Example

```{r data}
library(dkge)
S <- 4; q <- 4; P <- 30; T <- 80

betas <- replicate(S, matrix(rnorm(q * P, sd = 0.5), q, P), simplify = FALSE)
true_kernel <- 0.9 * (diag(q) + 0.3)
cholK <- chol(true_kernel)
betas <- lapply(betas, function(B) cholK %*% B + 0.1 * matrix(rnorm(q * P), q, P))

designs <- replicate(S, {
  X <- matrix(rnorm(T * q), T, q)
  qr.Q(qr(X))
}, simplify = FALSE)

subjects <- lapply(seq_len(S), function(s) dkge_subject(betas[[s]], designs[[s]], id = paste0("sub", s)))
bundle <- dkge_data(subjects)
```

`dkge_data()` aligns effect ordering and caches common metadata. The kernel we
supply next controls how strongly effects are smoothed or grouped.

## Building Kernels with `design_kernel()`

```{r kernels}
factors <- list(
  A = list(L = 2, type = "nominal"),
  B = list(L = 2, type = "nominal")
)
terms <- list("A", "B", c("A", "B"))
K_struct <- design_kernel(
  factors,
  terms = terms,
  rho = c("A" = 1, "B" = 1, "A:B" = 0.4),
  basis = "cell",
  normalize = "unit_trace"
)

round(K_struct$K, 2)
```

- Each term introduces a block-similarity component.
- `rho` weights the term; defaults to 1. Combine multiple terms to encode
  interactions or ordered trends.
- `design_kernel()` returns both the kernel matrix and metadata used by CV
  helpers below.

## Rank and Kernel Tuning via Cross-Validation

`dkge_cv_kernel_rank()` evaluates candidate ranks and kernels using
leave-one-subject-out objectives. Build a small grid of kernels and let the
helper pick the best combination.

```{r kernel-cv, message=FALSE}
rho_vals <- seq(0.4, 1.0, by = 0.3)
K_grid <- list()
for (ra in rho_vals) {
  for (rb in rho_vals) {
    nm <- sprintf("A%.1f_B%.1f", ra, rb)
    K_grid[[nm]] <- design_kernel(
      factors,
      terms = terms,
      rho = c("A" = ra, "B" = rb, "A:B" = 0.4),
      basis = "cell",
      normalize = "unit_trace"
    )$K
  }
}

cv_rows <- lapply(names(K_grid), function(nm) {
  cv <- dkge_cv_rank_loso(bundle$betas, bundle$designs, K_grid[[nm]], ranks = 1:3)
  cbind(kernel = nm, cv$table)
})
cv_table <- do.call(rbind, cv_rows)
head(cv_table)
```

The score reported by `dkge_cv_kernel_rank()` is the average reconstruction
error of left-out subjects in the K-metric. Lower is better. Pick the
rank/kernel combination with the highest cross-validated score and refit.

```{r refit}
best_idx <- which.max(cv_table$mean)
best <- cv_table[best_idx, ]
best

K_best <- K_grid[[best$kernel]]
fit <- dkge(bundle, kernel = K_best, rank = best$param)
round(fit$sdev, 3)
```

## Penalising Noisy Effects

When certain effects are known to be high variance (e.g. motion regressors), add
diagonal ridge terms via `dkge(..., ridge = value)` or pre-scale the kernel.
Higher ridge values shrink small singular values and stabilise fits when subject
counts are limited.

```{r ridge}
fit_ridge <- dkge(bundle, kernel = K_best, rank = best$param, ridge = 0.2)
round(fit_ridge$sdev, 3)
```

## Summary

- Use `design_kernel()` to encode your scientific priors; start with `diag(q)`
  if in doubt, then add structured terms gradually.
- Run `dkge_cv_kernel_rank()` (LOSO by default) to select rank and kernel before
  expensive inference.
- Add ridge regularisation when singular values drop sharply or subjects are
  few; check `fit$sdev` to verify stability.
